{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人脸关键点检测\n",
    "\n",
    "该项目是关于定义和训练用于执行人脸关键点检测的卷积神经网络，并使用计算机视觉技术来转换人脸图像。项目的第一步是加载和可视化将使用的数据。\n",
    "\n",
    "首先，我们来看一些图像和相应的人脸关键点示例。\n",
    "\n",
    "<img src='images/key_pts_example.png' width=50% height=50%/>\n",
    "\n",
    "人脸关键点（也称为人脸特征点）指的是上面的图像中，每个人脸上显示的洋红色的小点。在每个训练和测试图像中，有一个人脸和**68 个关键点，其中，人脸的坐标是 (x, y)**。这些关键点标记了人脸的重要区域：眼睛，嘴角，鼻子等。这些关键点与许多应用相关，如人脸滤波、情感识别、姿势识别等。在这里，它们是编号的，你可以看到特定范围的点与该人脸的不同部分相匹配。\n",
    "\n",
    "<img src='images/landmarks_numbered.jpg' width=30% height=30%/>\n",
    "\n",
    "---\n",
    "\n",
    "## 加载和可视化数据\n",
    "\n",
    "使用任何数据集的第一步，都是要熟悉你的数据。此外，你还需要加载人脸及其关键点的图像并将其可视化！这组图像数据是从[YouTube 人脸数据集](https://www.cs.tau.ac.il/~wolf/ytfaces/)中提取的，其中包含YouTube视频中的人物视频。这些视频通过一些处理步骤进行输入，并转换为包含一个人脸和相关关键点的图像帧集。\n",
    "\n",
    "#### 训练数据和测试数据\n",
    "\n",
    "该人脸关键点数据集由5770张彩色图像组成。所有这些图像都被分成训练数据集与测试数据集。\n",
    "\n",
    "* 这些图像中有3462张个是训练图像，供你在创建用来预测关键点的模型时使用。\n",
    "* 另外2308张是测试图像，用于测试该模型的准确性。\n",
    "\n",
    "有关此数据集中图像和关键点的信息汇总在CSV文件中，我们可以使用`pandas`读取这些文件。接下来，我们要读取训练CSV并在（N，2）数组中获取注释，其中N是关键点的数量，2是关键点坐标（x，y）的维度。\n",
    "\n",
    "---\n",
    "\n",
    "首先，在着手行动之前，必须要加载图像数据。这些数据存储在一个压缩文件中。在下面的单元格中，我们可以通过它的URL访问该压缩文件，并将数据解压缩到与工作区Home目录分开的`data/`目录中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载相应的包\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，加载训练数据，并显示有关该数据的一些统计数据，最后要确保它已正确加载！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:  (3462, 137)\n"
     ]
    }
   ],
   "source": [
    "key_pts_frame = pd.read_csv('data/training_frames_keypoints.csv')#读取关键点数据\n",
    "print('Number of images: ', key_pts_frame.shape)#查看关键点数据的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 观察一些图像\n",
    "\n",
    "下面是一个`show_keypoints`函数，它用于接收一张图像和关键点并将它们显示出来。查看此数据时，**请注意这些图像的尺寸不同，**人脸也不同！为了最终使用这些图像训练神经网络，我们需要标准化它们的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_keypoints(image, key_pts):\n",
    "    \"\"\"定义在图片上画关键点的函数\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(key_pts[:, 0], key_pts[:, 1], s=20, marker='.', c='m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python36\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#画张示例图片,可以通过n来改变图片\n",
    "n = 10\n",
    "image_name = key_pts_frame.iloc[n, 0]#获取图片名\n",
    "key_pts = key_pts_frame.iloc[n, 1:].as_matrix()#获取图片关键点的原始数据信息并转化为矩阵\n",
    "key_pts = key_pts.astype('float').reshape(-1, 2)#将图片关键点的原始数据转化为二维矩阵,列\n",
    "# 为每张图片的关键点的索引,行位关键点的在图片上的位置索引信息\n",
    "plt.figure(figsize=(5, 5))\n",
    "show_keypoints(mpimg.imread(os.path.join('data/training/', image_name)), key_pts)\n",
    "#调用show_keypoints函数,展示人脸的关键点\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset类与转换\n",
    "\n",
    "为了准备训练我们的数据，我们要使用PyTorch的Dataset类。这段代码大部分都是[PyTorch 数据加载教程](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html)中代码的修改版。\n",
    "\n",
    "#### Dataset类\n",
    "\n",
    "``torch.utils.data.Dataset``是一个表示数据集的抽象类。这个类可以让我们加载批量的图像/关键点数据，并统一地将转换应用于我们的数据，例如，为了训练神经网络，重新缩放和归一化化图像。\n",
    "\n",
    "\n",
    "你的自定义数据集应继承``Dataset``并覆盖以下方法：\n",
    "\n",
    "-  ``__len__`` ，从而使``len(dataset)``返回数据集的大小。\n",
    "\n",
    "-  ``__getitem__`` ，用于支持索引，使``dataset[i]`` 可\n",
    "     用于获取第i个图像/关键点数据样本。\n",
    "\n",
    "接下来，让我们为人脸关键点数据集创建一个dataset类。我们要读取``__init__``中的CSV文件，但将图像的读取留给``__getitem__``。这就是高效存储，因为所有图像都不是一次性存储在内存中，而是根据需要读取。\n",
    "\n",
    "我们的数据集示例将是一个字典``{'image': image, 'keypoints': key_pts}``。该数据集将采用可选参数``transform``，这样的话，任何所需的处理都可以应用于样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FacialKeypointsDataset(Dataset):\n",
    "    \"\"\"脸的关键点数据集\"\"\"\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            csv_file (string): 脸的关键点数据的储存路径.\n",
    "            root_dir (string): 图片的储存路径.\n",
    "            transform (callable, optional): 可选的参数,将图片转化为符合要求的图片格式\n",
    "        \"\"\"\n",
    "        self.key_pts_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.key_pts_frame)\n",
    "\n",
    "    def __getitem__(self, idx):#通过索引得到图片关键点信息\n",
    "        image_name = os.path.join(self.root_dir,self.key_pts_frame.iloc[idx, 0])\n",
    "        image = mpimg.imread(image_name)\n",
    "        # 去掉图片的透明度的通道\n",
    "        if(image.shape[2] == 4):\n",
    "            image = image[:,:,0:3]\n",
    "        key_pts = self.key_pts_frame.iloc[idx, 1:].as_matrix()\n",
    "        key_pts = key_pts.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'keypoints': key_pts}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们已经定义了这个类，接下来，我们要做的是实例化该数据集并显示一些图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python36\\lib\\site-packages\\ipykernel_launcher.py:25: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (173, 185, 3) (68, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (195, 176, 3) (68, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (308, 296, 3) (68, 2)\n"
     ]
    }
   ],
   "source": [
    "face_dataset = FacialKeypointsDataset(csv_file='data/training_frames_keypoints.csv',root_dir='data/training/')\n",
    "#示例3张图片\n",
    "num_to_display = 3\n",
    "for i in range(num_to_display):\n",
    "    # 定义图片的显示大小\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    #随机选择图片\n",
    "    rand_i = np.random.randint(0, len(face_dataset))\n",
    "    sample = face_dataset[rand_i]\n",
    "    # 打印图片的shape信息和关键点的shape信息\n",
    "    print(i, sample['image'].shape, sample['keypoints'].shape)\n",
    "    ax = plt.subplot(1, num_to_display, i + 1)\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    # 用show_keypoints函数显示图片\n",
    "    show_keypoints(sample['image'], sample['keypoints'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转换\n",
    "\n",
    "现在，上面的图像尺寸不同，但是，神经网络通常期望的是标准化的图像。因此，我们需要固定的尺寸、颜色范围和坐标的标准化范围。对于PyTorch来说，还需要把numpy列表和数组转换为Tensors。\n",
    "\n",
    "因此，我们需要编写一些预处理代码。\n",
    "下面，创建四个转换：\n",
    "\n",
    "-  ``Normalize``: 将彩色图像转换为范围为[0,1]的灰度值，并将关键点标准化为约[-1,1]的范围\n",
    "-  ``Rescale``: 将图像重新缩放到所需尺寸。\n",
    "-  ``RandomCrop``: 随机裁剪图像。\n",
    "-  ``ToTensor``: 将numpy图像转换为torch图像。\n",
    "\n",
    "\n",
    "我们将它们编写为可调用类而不是简单函数，这样，每次调用时都不需要传递转换的参数。 为此，我们只需要实现 ``__call__`` 方法就可以了。如果我们需要传入参数，还需要实现``__init__``方法。 我们可以使用类似下面的转换："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "",
    "\n",
    "请注意以下这些转换通常是如何应用于图像及其关键点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, utils\n",
    "class Normalize(object):\n",
    "    \"\"\"把彩色图片转换为灰度图片,并且标准化[0,1].\"\"\"        \n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "        image_copy = np.copy(image)\n",
    "        key_pts_copy = np.copy(key_pts)\n",
    "        image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)#转换为灰度图\n",
    "        image_copy=  image_copy/255.0 #标准化\n",
    "        #key_pts归一化到[-1, 1]\n",
    "        #平均值= 100,标准差= 50\n",
    "        key_pts_copy = (key_pts_copy - 100)/50.0\n",
    "        return {'image': image_copy, 'keypoints': key_pts_copy}\n",
    "class Rescale(object):\n",
    "    \"\"\"将图片转化为指定形状的图片.\n",
    "    参数:\n",
    "        output_size (tuple or int): 输出的图片形状,如果是tuple,则是转换为指定的形状\n",
    "        如果是int,则output_size对应的是较小的边,另一边乘以相应的比例\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        img = cv2.resize(image, (new_w, new_h))\n",
    "        # 对key_pts做相对应的处理\n",
    "        key_pts = key_pts * [new_w / w, new_h / h]\n",
    "        return {'image': img, 'keypoints': key_pts}\n",
    "class RandomCrop(object):\n",
    "    \"\"\"随机剪裁指定大小图片\n",
    "    Args:\n",
    "        output_size (tuple or int):指定大小,,如果是tuple,则是转换为指定的形状\n",
    "        如果是int,则output_size对应的是较小的边,另一边乘以相应的比例\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "        image = image[top: top + new_h,left: left + new_w]\n",
    "        key_pts = key_pts - [left, top]\n",
    "        return {'image': image, 'keypoints': key_pts}\n",
    "class ToTensor(object):\n",
    "    \"\"\"将ndarrays转化为ensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "        if(len(image.shape) == 2):\n",
    "            image = image.reshape(image.shape[0], image.shape[1], 1)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),'keypoints': torch.from_numpy(key_pts)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试转换\n",
    "\n",
    "接下来，需要对这些转换进行测试，确保它们按预期运行。查看每个转换时，请注意，在这里，**顺序非常重要**。例如，你不能用一个小于原始图像的值来裁剪图像，而且原始图像的尺寸会有所不同。但是，如果首先选择重新缩放原始图像，则可以将其裁剪为小于重新缩放尺寸的任何尺寸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python36\\lib\\site-packages\\ipykernel_launcher.py:25: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "rescale = Rescale(100)\n",
    "crop = RandomCrop(50)\n",
    "composed = transforms.Compose([Rescale(250),RandomCrop(224)])\n",
    "test_num = 500\n",
    "sample = face_dataset[test_num]\n",
    "fig = plt.figure()\n",
    "for i, tx in enumerate([rescale, crop, composed]):\n",
    "    transformed_sample = tx(sample)\n",
    "    ax = plt.subplot(1, 3, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title(type(tx).__name__)\n",
    "    show_keypoints(transformed_sample['image'], transformed_sample['keypoints'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建转换后的数据集\n",
    "\n",
    "下面，我们需要使用转换获取相同形状的灰度图像。通过输出结果数据的形状来验证转换的工作原理（输出的几个示例应该显示出一致的张量大小）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义tranform\n",
    "data_transform = transforms.Compose([Rescale(250),RandomCrop(224),Normalize(),ToTensor()])\n",
    "# 建立转化后的数据集\n",
    "transformed_dataset = FacialKeypointsDataset(csv_file='data/training_frames_keypoints.csv',root_dir='data/training/',transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 224, 224]) torch.Size([68, 2])\n1 torch.Size([1, 224, 224]) torch.Size([68, 2])\n2 torch.Size([1, 224, 224]) torch.Size([68, 2])\n3 torch.Size([1, 224, 224]) torch.Size([68, 2])\n4 torch.Size([1, 224, 224]) torch.Size([68, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python36\\lib\\site-packages\\ipykernel_launcher.py:25: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    sample = transformed_dataset[i]\n",
    "    print(i, sample['image'].size(), sample['keypoints'].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 此代码保存于data_load.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
