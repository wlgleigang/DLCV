{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 实战项目：图像标注\n",
    "\n",
    "在这个notebook中，训练CNN-RNN模型,搜索好的模型时尝试多种不同的架构和超参数。\n",
    "通过点击以下链接导航到该notebook：\n",
    "- [Step 1](#step1): 训练设置\n",
    "- [Step 2](#step2): 训练你的模型\n",
    "\n",
    "<a id='step1'></a>\n",
    "## Step 1: 训练设置\n",
    "\n",
    "在该notebook的此步骤中，需要通过定义超参数并设置训练过程中重要的其他选项来自定义对CNN-RNN模型的训练。\n",
    "### 任务 #1\n",
    "\n",
    "首先，请设置以下变量：\n",
    "- `batch_size` - 每个训练批次的批次大小。它是指用于在每个训练步骤中修改模型权重的图像标注对的数量。\n",
    "- `vocab_threshold` - 单词阈值最小值。请注意，阈值越大，词汇量越小，而阈值越小，则表示将包括较少的词汇，词汇量则越大。\n",
    "- `vocab_from_file` - 一个布尔值，用于决定是否从文件加载词汇表。\n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  图像和单词嵌入的维度。\n",
    "- `hidden_size` - RNN解码器隐藏状态下的特征数。\n",
    "- `num_epochs` - 训练模型的epoch数。我们建议你设置为`num_epochs=3`，但可以根据需要随意增加或减少此数字。 [这篇论文](https://arxiv.org/pdf/1502.03044.pdf) 在一个最先进的GPU上对一个标注生成模型训练了3天，但很快你就会发现，其实在几个小时内就可以得到合理的结果！（_但是，如果你想让你的模型与当前的研究一较高下，则需要更长时间的训练。_)\n",
    "- `save_every` - 确定保存模型权重的频率。我们建议你设置为`save_every=1`，便于在每个epoch后保存模型权重。这样，在第`i`个epoch之后，编码器和解码器权重将在`models/`文件夹中分别保存为`encoder-i.pkl`和`decoder-i.pkl`。\n",
    "- `print_every` - 确定在训练时将批次损失输出到Jupyter notebook的频率。请注意，训练时，你**将不会**看到损失函数的单调减少，这一点非常好并且完全可以预料到！我们建议你将其保持在默认值`100` ，从而避免让这个notebook运行变慢，但之后随时都可以进行更改。\n",
    "- `log_file` - 包含每个步骤中训练期间的损失与复杂度演变过程的的文本文件的名称。\n",
    "\n",
    "对于上述某些值，可以仔细阅读 [这篇文章](https://arxiv.org/pdf/1502.03044.pdf) 与 [这篇文章](https://arxiv.org/pdf/1411.4555.pdf) ，获得有用的指导！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.81s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1027/414113 [00:00<01:25, 4845.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:10<00:00, 5867.67it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "batch_size =128          # 批量\n",
    "vocab_threshold = 5.        # 阈值\n",
    "vocab_from_file = True  # 是否使用词汇字典文件\n",
    "embed_size =512           # 词嵌入大小\n",
    "hidden_size = 512          #隐藏层大小\n",
    "num_epochs = 3             # epochs\n",
    "save_every = 1             \n",
    "print_every = 100        \n",
    "log_file = 'training_log.txt'       # 保存训练过程的数据\n",
    "\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256), \n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),              \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())+ list(encoder.bn.parameters())\n",
    "optimizer=torch.optim.Adam(params)\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: 训练你的模型\n",
    "\n",
    "在**Step 1**中执行代码单元格后，下面的训练过程应该就不会出现问题了。\n",
    "使用加载已保存的权重来恢复训练很有用。使用下面的代码行加载权重：\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "\n",
    "### 关于调整超参数的说明\n",
    "\n",
    "但是，这样无法知道模型是否过度拟合训练数据,过度拟合是训练图像标注模型时常会遇到的问题。可以阅读 [本文](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636)4.3.1节中最小化过度拟合的一些方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/3236], Loss: 4.1022, Perplexity: 60.4745\n",
      "Epoch [1/3], Step [200/3236], Loss: 3.3935, Perplexity: 29.7695\n",
      "Epoch [1/3], Step [300/3236], Loss: 3.2420, Perplexity: 25.58538\n",
      "Epoch [1/3], Step [400/3236], Loss: 2.9838, Perplexity: 19.7632\n",
      "Epoch [1/3], Step [500/3236], Loss: 2.9309, Perplexity: 18.7439\n",
      "Epoch [1/3], Step [600/3236], Loss: 3.1366, Perplexity: 23.0264\n",
      "Epoch [1/3], Step [700/3236], Loss: 2.7119, Perplexity: 15.05797\n",
      "Epoch [1/3], Step [800/3236], Loss: 2.5724, Perplexity: 13.0969\n",
      "Epoch [1/3], Step [900/3236], Loss: 3.4026, Perplexity: 30.0429\n",
      "Epoch [1/3], Step [1000/3236], Loss: 2.5100, Perplexity: 12.3044\n",
      "Epoch [1/3], Step [1100/3236], Loss: 2.3984, Perplexity: 11.0054\n",
      "Epoch [1/3], Step [1200/3236], Loss: 2.6150, Perplexity: 13.6675\n",
      "Epoch [1/3], Step [1300/3236], Loss: 2.4940, Perplexity: 12.1098\n",
      "Epoch [1/3], Step [1400/3236], Loss: 2.4189, Perplexity: 11.2337\n",
      "Epoch [1/3], Step [1500/3236], Loss: 2.4296, Perplexity: 11.3548\n",
      "Epoch [1/3], Step [1600/3236], Loss: 2.5036, Perplexity: 12.2261\n",
      "Epoch [1/3], Step [1700/3236], Loss: 2.5883, Perplexity: 13.3074\n",
      "Epoch [1/3], Step [1800/3236], Loss: 2.2366, Perplexity: 9.36157\n",
      "Epoch [1/3], Step [1900/3236], Loss: 2.5051, Perplexity: 12.2451\n",
      "Epoch [1/3], Step [2000/3236], Loss: 2.3117, Perplexity: 10.0914\n",
      "Epoch [1/3], Step [2100/3236], Loss: 2.3602, Perplexity: 10.5928\n",
      "Epoch [1/3], Step [2200/3236], Loss: 2.3966, Perplexity: 10.9859\n",
      "Epoch [1/3], Step [2300/3236], Loss: 2.2836, Perplexity: 9.81210\n",
      "Epoch [1/3], Step [2400/3236], Loss: 2.2043, Perplexity: 9.06388\n",
      "Epoch [1/3], Step [2500/3236], Loss: 2.2480, Perplexity: 9.46881\n",
      "Epoch [1/3], Step [2600/3236], Loss: 2.0623, Perplexity: 7.86402\n",
      "Epoch [1/3], Step [2700/3236], Loss: 2.1662, Perplexity: 8.72486\n",
      "Epoch [1/3], Step [2800/3236], Loss: 2.7335, Perplexity: 15.3866\n",
      "Epoch [1/3], Step [2900/3236], Loss: 2.5061, Perplexity: 12.2568\n",
      "Epoch [1/3], Step [3000/3236], Loss: 2.4124, Perplexity: 11.1609\n",
      "Epoch [1/3], Step [3100/3236], Loss: 2.2544, Perplexity: 9.52961\n",
      "Epoch [1/3], Step [3200/3236], Loss: 2.3178, Perplexity: 10.1536\n",
      "Epoch [2/3], Step [100/3236], Loss: 2.2942, Perplexity: 9.916082\n",
      "Epoch [2/3], Step [200/3236], Loss: 2.3232, Perplexity: 10.2082\n",
      "Epoch [2/3], Step [300/3236], Loss: 2.1600, Perplexity: 8.67131\n",
      "Epoch [2/3], Step [400/3236], Loss: 2.2667, Perplexity: 9.64748\n",
      "Epoch [2/3], Step [500/3236], Loss: 2.1025, Perplexity: 8.18684\n",
      "Epoch [2/3], Step [600/3236], Loss: 2.0806, Perplexity: 8.00930\n",
      "Epoch [2/3], Step [700/3236], Loss: 2.1619, Perplexity: 8.68787\n",
      "Epoch [2/3], Step [800/3236], Loss: 2.1592, Perplexity: 8.66459\n",
      "Epoch [2/3], Step [900/3236], Loss: 2.2853, Perplexity: 9.82897\n",
      "Epoch [2/3], Step [1000/3236], Loss: 2.3964, Perplexity: 10.9836\n",
      "Epoch [2/3], Step [1100/3236], Loss: 2.1872, Perplexity: 8.91049\n",
      "Epoch [2/3], Step [1200/3236], Loss: 2.2050, Perplexity: 9.07037\n",
      "Epoch [2/3], Step [1300/3236], Loss: 2.0737, Perplexity: 7.95428\n",
      "Epoch [2/3], Step [1400/3236], Loss: 2.3060, Perplexity: 10.0347\n",
      "Epoch [2/3], Step [1500/3236], Loss: 2.2064, Perplexity: 9.08300\n",
      "Epoch [2/3], Step [1600/3236], Loss: 2.5713, Perplexity: 13.0830\n",
      "Epoch [2/3], Step [1700/3236], Loss: 2.2678, Perplexity: 9.65808\n",
      "Epoch [2/3], Step [1800/3236], Loss: 2.1939, Perplexity: 8.97009\n",
      "Epoch [2/3], Step [1900/3236], Loss: 2.1156, Perplexity: 8.29487\n",
      "Epoch [2/3], Step [2000/3236], Loss: 2.2969, Perplexity: 9.94362\n",
      "Epoch [2/3], Step [2100/3236], Loss: 2.0758, Perplexity: 7.97127\n",
      "Epoch [2/3], Step [2200/3236], Loss: 2.1820, Perplexity: 8.86416\n",
      "Epoch [2/3], Step [2300/3236], Loss: 2.1258, Perplexity: 8.37960\n",
      "Epoch [2/3], Step [2400/3236], Loss: 2.2016, Perplexity: 9.03953\n",
      "Epoch [2/3], Step [2500/3236], Loss: 2.1252, Perplexity: 8.37452\n",
      "Epoch [2/3], Step [2600/3236], Loss: 2.1793, Perplexity: 8.84039\n",
      "Epoch [2/3], Step [2700/3236], Loss: 1.9344, Perplexity: 6.91973\n",
      "Epoch [2/3], Step [2800/3236], Loss: 2.0752, Perplexity: 7.96618\n",
      "Epoch [2/3], Step [2900/3236], Loss: 2.0981, Perplexity: 8.15090\n",
      "Epoch [2/3], Step [3000/3236], Loss: 1.8954, Perplexity: 6.655120\n",
      "Epoch [2/3], Step [3100/3236], Loss: 1.9649, Perplexity: 7.13436\n",
      "Epoch [2/3], Step [3200/3236], Loss: 2.0112, Perplexity: 7.47260\n",
      "Epoch [3/3], Step [100/3236], Loss: 2.1638, Perplexity: 8.704316\n",
      "Epoch [3/3], Step [200/3236], Loss: 2.0604, Perplexity: 7.84890\n",
      "Epoch [3/3], Step [300/3236], Loss: 1.9949, Perplexity: 7.35178\n",
      "Epoch [3/3], Step [400/3236], Loss: 1.9854, Perplexity: 7.28213\n",
      "Epoch [3/3], Step [500/3236], Loss: 2.0042, Perplexity: 7.42031\n",
      "Epoch [3/3], Step [600/3236], Loss: 2.0759, Perplexity: 7.97216\n",
      "Epoch [3/3], Step [700/3236], Loss: 2.1308, Perplexity: 8.42166\n",
      "Epoch [3/3], Step [800/3236], Loss: 2.0112, Perplexity: 7.47190\n",
      "Epoch [3/3], Step [900/3236], Loss: 2.3617, Perplexity: 10.6094\n",
      "Epoch [3/3], Step [1000/3236], Loss: 2.0997, Perplexity: 8.1637\n",
      "Epoch [3/3], Step [1100/3236], Loss: 1.9838, Perplexity: 7.27057\n",
      "Epoch [3/3], Step [1200/3236], Loss: 2.0692, Perplexity: 7.91872\n",
      "Epoch [3/3], Step [1300/3236], Loss: 2.0850, Perplexity: 8.04468\n",
      "Epoch [3/3], Step [1400/3236], Loss: 1.9890, Perplexity: 7.30799\n",
      "Epoch [3/3], Step [1500/3236], Loss: 2.3979, Perplexity: 11.0000\n",
      "Epoch [3/3], Step [1600/3236], Loss: 2.0222, Perplexity: 7.55480\n",
      "Epoch [3/3], Step [1700/3236], Loss: 2.2447, Perplexity: 9.43775\n",
      "Epoch [3/3], Step [1800/3236], Loss: 2.0188, Perplexity: 7.52906\n",
      "Epoch [3/3], Step [1900/3236], Loss: 1.9535, Perplexity: 7.05331\n",
      "Epoch [3/3], Step [2000/3236], Loss: 2.0674, Perplexity: 7.90416\n",
      "Epoch [3/3], Step [2100/3236], Loss: 2.0604, Perplexity: 7.84929\n",
      "Epoch [3/3], Step [2200/3236], Loss: 2.2380, Perplexity: 9.37458\n",
      "Epoch [3/3], Step [2300/3236], Loss: 2.0677, Perplexity: 7.90671\n",
      "Epoch [3/3], Step [2400/3236], Loss: 2.0137, Perplexity: 7.49101\n",
      "Epoch [3/3], Step [2500/3236], Loss: 2.1206, Perplexity: 8.33658\n",
      "Epoch [3/3], Step [2557/3236], Loss: 2.0104, Perplexity: 7.46668"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for i_step in range(1, total_step+1):\n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        images, captions = next(iter(data_loader))\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
